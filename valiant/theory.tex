The classical dynamic {\sc cyk} algorithm for parsing context-free grammars runs in cubic time (in terms of the input word). 
In this chapter we present a parsing algorithm of  Valiant~\cite{Valiant:1975bn}, which parses  context-free languages  in approximately the same time as matrix multiplication.  The matrices are  Boolean, which means that the entries are $0$ or $1$, addition  is $\lor$ and multiplication is $\land$. For readers wary of matrices, an $n \times m$ Boolean matrix is the same as a binary relation between $\set{1,\ldots,n}$ and $\set{1,\ldots,m}$, and matrix multiplication is composition of relations, as in  this picture:
	\mypicf{4}
The naive algorithm for matrix multiplication runs in time $n^3$, but smarter algorithms run faster, e.g.~the Strassen algorithm runs in time approximately $\Oo(n^{2.8704})$, and the record holder as of  2024 is $\Oo(n^{2.3715})$, see~\cite{williams2024new}. The exponent in the running time of matrix multiplication is denoted by $\omega$. We know that this value is at least $2$, because one needs to read the matrices, and currently it is known to be at most $2.3715$.  The purpose of this chapter is to explain an algorithm, due to Valiant, which employs matrix multiplication to parse context-free languages in sub-cubic time.  The Valiant algorithm is not practical for parsing, because the constant factors are large in the fast matrix multiplication algorithms, but it is a milestone in the theory of algorithms.

\begin{theorem}
	\label{thm:valiant}
	Assume that  multiplication of $n \times n$ Boolean matrices can be computed in time $\Oo(n^\omega)$ for some real number $\omega$. Then membership in a context-free language can be decided in time at most
	\begin{align*}
\mathrm{poly}(\Gg) \cdot n^\omega \cdot \log(n) \qquad \text{where $\Gg$ is the grammar and $n$ is the length of the input.}\end{align*}
\end{theorem}
For $\omega > 2$, the running time can be further improved to
$
\mathrm{poly}(\Gg) \cdot n^\omega
$
which we justify later in footnote \ref{footnote:log}.

For the rest of this chapter, fix $\omega$ and a context-free grammar $\Gg$. We assume that the grammar is in Chomsky Normal Form, i.e.~every rule is of the form $X \leftarrow YZ$ or $X \leftarrow a$, where $X,Y,Z$ are nonterminals and $a$ is a terminal. A grammar can be converted into Chomsky Normal Form in polynomial time, so this assumption can be made without loss of generality. 
\newcommand{\ptrip}[3]{#1 \stackrel {#2} \to #3}

The main data structure that will be used in this algorithm is what we call a parse matrix for some input string. Define an \emph{interval} in an input string to be a connected sequence of positions. We think of an interval as connecting two cuts (i.e.~spaces between positions) in an input string, as in the following picture:
\mypicf{6}
A parse matrix is a collection of facts of the form  ``the infix at interval $I$  can be generated by a nonterminal $X$''. We always want this collection to be \emph{sound}, i.e.~every fact in the parse matrix should actually be true, but it does not need to be \emph{complete}, which means that the parse matrix contains all true facts. The information about nonterminal $X$ in a parse matrix can be seen as a Boolean matrix $M_X$ where the rows are source cuts, and the columns are target cuts. All nonzero entries will be strictly above the diagonal, since the source cut must be strictly before the target cut. (Since we do not have $\epsilon$-productions, every nonterminal generates only nonempty strings). A parse matrix for a string of length $n$ is called a \emph{parse matrix of length $n$}. In this parse matrix, the underlying Boolean matrices for each nonterminal have dimension $(n+1) \times (n+1)$, since there are $n+1$ cuts.  


As mentioned above, a parse matrix can be viewed as a collection $M = \set{M_X}_X$ of Boolean matrices, indexed by nonterminals. 
For parse matrices $M,N$ of same length, define their product $M \cdot N$ by
\begin{align*}
(M \cdot N)_X \eqdef \bigcup_{X \to YZ} M_Y \cdot M_Z
\end{align*}
where the union ranges over rules of the grammar and $M_Y \cdot M_Z$ is matrix multiplication. The product consists of facts of the form ``interval $I$ can be generated by $X$'', which can be derived by taking rule $X \to YZ$ in the grammar, and a decomposition of $I$ into two intervals $J$ and $K$, such that the matrix $M$ contains that fact ``interval $J$ can be generated by $Y$'' and the matrix $N$ contains that fact ``interval $K$ can be generated by $Z$''. Since we have described product of parse matrices using matrix multiplication, we get the following observation.

\begin{lemma}\label{lem:matrix-mult}
	For length $n$ parse matrices, product can be computed in time $\Oo(n^\omega)$.
\end{lemma}
We say that a parse matrix $M$ is \emph{closed} if it satisfies $M \cdot M \subseteq M$, and we say that it is closed  on an interval $I$ if it is closed when restricted to intervals contained in $I$.  For a parse matrix $M$, define its \emph{closure} $M^*$ to be the least (with respect to inclusion) parse matrix that contains $M$ and is closed.


\begin{proposition}\label{prop:combine-halves}
There is an algorithm which runs in time
\begin{align*}
T(n) \leq \mathrm{poly}(\Gg) \cdot \log(n) \cdot n^\omega
\end{align*}
and which computes the closure of a length $2n$ parse matrix,  assuming that it is closed on the intervals $\set{1,\ldots,n}$ and $\set{n+1,\ldots,2n}$.
\end{proposition}

Before proving the proposition, we show how it implies the Theorem~\ref{thm:valiant}.

\begin{proof}[Proof of Theorem~\ref{thm:valiant}]
Suppose that we want to know if the grammar $\Gg$ generates a word $w$ of length $n$. Define $M$ to be the length $n$ parse matrix where $M_X$ contains intervals $\set{i}$ such that nonterminal $X$ generates the $i$-th letter of $w$, using a rule of the form $X \to a$.
This parse matrix can be computed in time linear in $n$. 
The word $w$ is generated by the grammar if and only if the closure $M^*$ contains the full interval on the component corresponding to the starting nonterminal. It suffices therefore to compute the closure $M^*$. To make the computation easier, suppose that the length of the word is a power of two, i.e.~$n=2^k$. We do a divide an conquer approach: we compute the closures of the parse matrix for the first and second halves of $w$  (using a recursive procedure), and then combine these using the algorithm from Proposition~\ref{prop:combine-halves}. The running time of this algorithm is at most
\begin{align}\label{eq:valiant-running-time}
  T(n) + 2T(\frac n 2) + \cdots + 2^k T(\frac n {2^k}).
\end{align}	
Because $T(n)$ is at least quadratic, it follows that
\begin{align*}
  2^i T(\frac n {2^i}) \le 2^i \frac{T(n)}{\left(2^i\right)^2} = \frac{T(n)}{2^i},
\end{align*}
so the above sum is bounded by
\begin{align*}
  T(n) + \frac{T(n)}{2} + \cdots + \frac{T(n)}{2^k} < 2 T(n),
\end{align*}
which shows that the running time~\eqref{eq:valiant-running-time} is at most two times slower than $T(n)$, thus proving the theorem, given the bounds on $T(n)$ from Proposition~\ref{prop:combine-halves}. \end{proof}

It remains to prove the proposition. We use the following lemma.



\begin{lemma}\label{lem:overlap}
Suppose that $M$ is a length $k+2n$ parse matrix that is closed on the intervals $A \cup B$ and $B \cup C$ as depicted below: \mypic{59}
Then the closure $M^*$  can be computed in time 
 $ \mathrm{poly}(\Gg)  \cdot n^\omega + T(n)$.
\end{lemma}
\begin{proof}  
Define $N$ to be $M \cup M \cdot M$ restricted to intervals that contain $B$ or are disjoint with $B$. Here, the sum takes the union of all facts stored in the two parse matrices.    The main observation in the lemma is
the following claim.
\begin{claim}
	$M^* = M \cup N^*$.
\end{claim}

Before proving the claim, we note that the right side of the above equality can be computed in time as in the statement of the lemma, thus proving the lemma.   By Lemma~\ref{lem:matrix-mult}, the parse matrix $N$ can be computed in time $\mathrm{poly}(\Gg)  \cdot n^\omega$, using matrix multiplication for the product $M \cdot M$. Because the matrix $M$ is closed over intervals $A$ and $C$, it follows that $N$ is also closed over these intervals. Since all entries of $N$ contain $B$ or are disjoint with $B$, it is essentially a matrix of length $2n$ whose first and second halves are closed. It follows that $N^*$ can be computed in time $T(n)$. 


It remains to prove the claim. The inclusion $\supseteq$ is immediate, it remains to justify the inclusion $\subseteq$.  
We need to show that if $M^*$ contains interval $I$ on nonterminal $X$, then this is true for $M \cup N^*$. If $I$ is contained in $A \cup B$ or $B \cup C$, then this implication holds by  the closure assumptions on $M$. The remaining case is when $I$ contains $B$. The reason for $M^*$ containing $I$ on nonterminal $X$  is  a parse tree as described in the following picture:
\mypicc{25}
In the parse tree, use red consider the smallest interval which contains $B$, and use yellow for the descendants of the red interval:
\mypicc{26} 
By minimality, each yellow interval is contained in either  $A \cup B$ or  $B \cup C$, and therefore belongs to  $M$ by the closure assumptions on $M$.  Therefore, the red itself belongs to $M \cdot M$. The red interval contains $B$, and the blue intervals are disjoint with $B$, therefore the red and blue intervals are in $N$. It follows that the red and blue intervals form a parse tree corresponding  to the matrix  $N^*$. 
\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:combine-halves}]
	Here is the algorithm. Suppose that $M$ is a length $2n$ parse matrix which is closed on its first and second halves, as in the assumption of the proposition. Let us write $A,B,C,D$ for the intervals describing the four quarters of $2n$, as in the following picture: \mypic{55}
As in Lemma~\ref{lem:overlap},	the blue rectangles indicate the intervals which are closed. 
	\begin{enumerate}
	\item By induction,  compute the closure of the interval $B \cup C$:
	\mypic{56}
		\item 	Using Lemma~\ref{lem:overlap} twice,   compute the closures of $A \cup B \cup C$ and  $B \cup C \cup D$:
\mypic{57}
		\item Using Lemma~\ref{lem:overlap}, compute the closure of $A \cup B \cup C \cup D$:
\mypic{58}
	\end{enumerate}
The cost of the above procedure is:
\begin{align*}
  T(n) = \underbrace{T(n/2)}_{\text{step 1}} +  \underbrace{2 \cdot (T(n/2) + c \cdot n^\omega)}_{\text{step 2}} + \underbrace{T(n/2) +  c \cdot n^\omega}_{\text{step 3}} 
\end{align*}
for some $c$ polynomial in the grammar. Summing up,
\begin{align*}
T(n) =   4T(n/2) + 3c \cdot n^\omega.
\end{align*}
Reasoning as in the end of the proof of Theorem~\ref{thm:valiant}, we get
\begin{align*}
  T(n) = 3c \cdot n^\omega + 4 \cdot 3c \cdot (\frac n 2)^\omega + \cdots + 4^k \cdot 3c \cdot (\frac n {2^k})^\omega.
\end{align*}	
Because $n^\omega$ is at least quadratic (an algorithm for matrix multiplication must at least read two  $n \times n$ matrices), it follows that 
\begin{align*}
  4^i  \cdot (\frac n {2^i})^\omega \le     n^\omega,
\end{align*}
which gives the bound in the proposition.
\footnote{
\label{footnote:log}
Assuming that $\omega > 2$, we can get rid of the $\log(n)$ in the running time, because the sum
$\sum_{i=0}^{k} 4^i (\frac{n}{2^i})^{\omega}$
is bounded by
$n^{\omega}\sum_{i=0}^{\infty} (\frac{1}{2^{\omega - 2}})^{i}$
which is $\O(n^{\omega})$, because
$\frac{1}{2^{\omega - 2}} < 1$.
}
\end{proof}

